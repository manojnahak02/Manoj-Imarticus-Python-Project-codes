{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSupervised learning algorithm (pre defined target variable)\\nCan be used for classification and regression\\nMostly used for classification\\nIt splits the population or sample into two or more homogeneous sets on the basis of the most significant splitter\\nSuitable for large number of classes in a categorical variable\\n\\n\\nTypes of Decision Trees \\n-Categorical variable DT\\n-Continuous variable DT\\n\\nTerminalogy\\n-Root node : represents the entire population. Starting point.\\n-Decision node: sub-node which further splits into sub-nodes\\n-Terminal/Leaf node: nodes do not split any further\\n-Splitting: process of dividing the node\\n-Pruning: remove sub-nodes or restricting the tree size to prevent over-fitting\\n-Branch/sub-tree\\n-Parent and child node\\n\\nAdvantages:\\n-easy to understand\\n-useful in data exploration\\n-less data cleaning required\\n-data type is not a constraint (can handle both numerical and categorical variables)\\n-non-parametric method (DT have no assumptions)\\n\\nDisadvantages:\\n-Over-fitting\\n-Not fit for continuos variables\\n\\nModel building\\n1) when dependent variable is categorical in nature then it is a classification tree,\\nwhereas when dependent variable is continuous in nature then it is a regression tree\\n2) In case of classification tree, the value obtained by terminal node in the training data is the mode of the observations falling in that region.\\nthus if an unseen data observations falls in that region, we'll make its prediction with mode value.\\n3) In case of regression tree, the value obtained by terminal node in the training data is the mean of the observations falling in that region.\\nthus if an unseen data observations falls in that region, we'll make its prediction with mean value.\\n4) It is a greedy algorithm, it focuses on the current split and not about the future plit\\n\\n\\n##Criteria for Classification DT:\\n\\n1)Gini Index (criteria for a classifier DT)\\n-works with categorical target variable\\n-performs binary split\\n-higher the value of gini higher is the homogeneity\\n-Classification and regression trees uses gini method to create binary splits\\n\\n2)Chi square\\n-Another technique that can be used for classification DT\\n-higher the chi-sq value, higher is the homogeneity\\n\\n3)Information Gain\\n-where the outcome is known with some certainity then the entropy is low, whereas if the outcome is uncertain then the entropy is high\\n-variable with low entropy value will give higher homogeneity\\nInformation gain = 1-Entropy\\n\\n\\n##Criteria for Regressor DT:\\n1)Reduction in variance: it tries to split the node on the basis of lower variance value.\\n\\n\\n\\nPruning the DT\\n-- setting constraints on DT\\n1) min samples for node split (default value = 0)\\n2) min samples for terminal node split (default value = 1)\\n3) max depth of the tree\\n4) max number of terminal nodes\\n5) max features to consider for split\\n\\n-- ensemble modelling\\n\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Supervised learning algorithm (pre defined target variable)\n",
    "Can be used for classification and regression\n",
    "Mostly used for classification\n",
    "It splits the population or sample into two or more homogeneous sets on the basis of the most significant splitter\n",
    "Suitable for large number of classes in a categorical variable\n",
    "\n",
    "\n",
    "Types of Decision Trees \n",
    "-Categorical variable DT\n",
    "-Continuous variable DT\n",
    "\n",
    "Terminalogy\n",
    "-Root node : represents the entire population. Starting point.\n",
    "-Decision node: sub-node which further splits into sub-nodes\n",
    "-Terminal/Leaf node: nodes do not split any further\n",
    "-Splitting: process of dividing the node\n",
    "-Pruning: remove sub-nodes or restricting the tree size to prevent over-fitting\n",
    "-Branch/sub-tree\n",
    "-Parent and child node\n",
    "\n",
    "Advantages:\n",
    "-easy to understand\n",
    "-useful in data exploration\n",
    "-less data cleaning required\n",
    "-data type is not a constraint (can handle both numerical and categorical variables)\n",
    "-non-parametric method (DT have no assumptions)\n",
    "\n",
    "Disadvantages:\n",
    "-Over-fitting\n",
    "-Not fit for continuos variables\n",
    "\n",
    "Model building\n",
    "1) when dependent variable is categorical in nature then it is a classification tree,\n",
    "whereas when dependent variable is continuous in nature then it is a regression tree\n",
    "2) In case of classification tree, the value obtained by terminal node in the training data is the mode of the \n",
    "observations falling in that region, thus if an unseen data observations falls in that region, we'll make its prediction \n",
    "with mode value.\n",
    "3) In case of regression tree, the value obtained by terminal node in the training data is the mean of the observations \n",
    "falling in that region. thus if an unseen data observations falls in that region, we'll make its prediction with mean value.\n",
    "4) It is a greedy algorithm, it focuses on the current split and not about the future plit\n",
    "\n",
    "\n",
    "##Criteria for Classification DT:\n",
    "\n",
    "1)Gini Index (criteria for a classifier DT)\n",
    "-works with categorical target variable\n",
    "-performs binary split\n",
    "-higher the value of gini higher is the homogeneity\n",
    "-Classification and regression trees uses gini method to create binary splits\n",
    "\n",
    "2)Chi square\n",
    "-Another technique that can be used for classification DT\n",
    "-higher the chi-sq value, higher is the homogeneity\n",
    "\n",
    "3)Information Gain\n",
    "-where the outcome is known with some certainity then the entropy is low, whereas if the outcome is uncertain then the \n",
    "entropy is high\n",
    "-variable with low entropy value will give higher homogeneity\n",
    "Information gain = 1-Entropy\n",
    "\n",
    "\n",
    "##Criteria for Regressor DT:\n",
    "1)Reduction in variance: it tries to split the node on the basis of lower variance value.\n",
    "\n",
    "\n",
    "\n",
    "Pruning the DT\n",
    "-- setting constraints on DT\n",
    "1) min samples for node split (default value = 0)\n",
    "2) min samples for terminal node split (default value = 1)\n",
    "3) max depth of the tree\n",
    "4) max number of terminal nodes\n",
    "5) max features to consider for split\n",
    "\n",
    "-- ensemble modelling\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1  2  3      4     5      6\n",
       "0  vhigh  vhigh  2  2  small   low  unacc\n",
       "1  vhigh  vhigh  2  2  small   med  unacc\n",
       "2  vhigh  vhigh  2  2  small  high  unacc\n",
       "3  vhigh  vhigh  2  2    med   low  unacc\n",
       "4  vhigh  vhigh  2  2    med   med  unacc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_data=pd.read_csv('cars.csv', header=None)\n",
    "cars_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1728, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_data.columns=['buying','maint','doors','persons','lug_boot','safety','classes'] ##assigning header for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety classes\n",
       "0  vhigh  vhigh     2       2    small    low   unacc\n",
       "1  vhigh  vhigh     2       2    small    med   unacc\n",
       "2  vhigh  vhigh     2       2    small   high   unacc\n",
       "3  vhigh  vhigh     2       2      med    low   unacc\n",
       "4  vhigh  vhigh     2       2      med    med   unacc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buying      0\n",
       "maint       0\n",
       "doors       0\n",
       "persons     0\n",
       "lug_boot    0\n",
       "safety      0\n",
       "classes     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_data.isnull().sum() ##no missing values\n",
    "#outlier treatement is not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df=pd.DataFrame.copy(cars_data) #creating a dataframe as a copy of the original data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Categorical data into numerical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'classes'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colname=cars_df.columns\n",
    "colname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all variables are catagorical so we will apply Label encoding to all X variables.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for x in colname:\n",
    "    cars_df[x]=le.fit_transform(cars_df[x]) #fit and transform are two steps of label encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   buying  maint  doors  persons  lug_boot  safety  classes\n",
       "0       3      3      0        0         2       1        2\n",
       "1       3      3      0        0         2       2        2\n",
       "2       3      3      0        0         2       0        2\n",
       "3       3      3      0        0         1       1        2\n",
       "4       3      3      0        0         1       2        2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nacc==>0\\ngood==>1\\nunacc==>2\\nvgood==>3\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_data.classes.value_counts()\n",
    "\"\"\"\n",
    "acc==>0\n",
    "good==>1\n",
    "unacc==>2\n",
    "vgood==>3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating X and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cars_df.values[:,:-1] #all rows and all columns except last one\n",
    "Y=cars_df.values[:,-1] #all rows and last column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Standard scaler reduces the data variations (sort of outlier treatment) \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "\n",
    "X=scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.34164079 -1.34164079 -0.4472136   1.22474487 -1.22474487 -1.22474487]\n",
      " [-0.4472136   1.34164079 -1.34164079  1.22474487  0.          0.        ]\n",
      " [ 0.4472136   1.34164079  0.4472136   1.22474487  1.22474487  0.        ]\n",
      " ...\n",
      " [-1.34164079  1.34164079  1.34164079  0.          0.         -1.22474487]\n",
      " [ 0.4472136   0.4472136   0.4472136   0.         -1.22474487  0.        ]\n",
      " [ 0.4472136  -0.4472136   1.34164079  1.22474487  1.22474487 -1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#less than 1800 then 70:30, if greater than 1800 then 80:20\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3, random_state=10) #70:30 split \n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Decision Tree Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=10,\\n            splitter='best')\\n            \\nHere criteria used is Gini index\\nmax_depth=None -- max depth of the tree is not defined\\nmax_features=None -- max features of the tree is not defined\\nmax_leaf_nodes=None -- max_leaf_nodes of the tree is not mentioned\\nspillter = best -- try finding the point on the basis of which the sample is split to get max homogeneity\\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting using the Decision Tree Classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_DecisionTree = DecisionTreeClassifier(random_state=10)\n",
    "\n",
    "#model_DecisionTree = DecisionTreeClassifier(random_state=10, min_samples_leaf=3, max_depth=10) #manually pruning the model\n",
    "\n",
    "\n",
    "#fit the model on the data and predict the values\n",
    "\n",
    "model_DecisionTree.fit(X_train,Y_train)\n",
    "\n",
    "\"\"\"\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=10,\n",
    "            splitter='best')\n",
    "            \n",
    "Here criteria used is Gini index\n",
    "max_depth=None -- max depth of the tree is not defined\n",
    "max_features=None -- max features of the tree is not defined\n",
    "max_leaf_nodes=None -- max_leaf_nodes of the tree is not mentioned\n",
    "spillter = best -- try finding the point on the basis of which the sample is split to get max homogeneity\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=model_DecisionTree.predict(X_test) #predicting Y values for X test data\n",
    "#print(Y_pred)\n",
    "#print(list(zip(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101   0   1   0]\n",
      " [  2  19   0   0]\n",
      " [  0   0 371   0]\n",
      " [  1   0   0  24]]\n",
      "\n",
      "\n",
      "0.9922928709055877\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       102\n",
      "           1       1.00      0.90      0.95        21\n",
      "           2       1.00      1.00      1.00       371\n",
      "           3       1.00      0.96      0.98        25\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       519\n",
      "   macro avg       0.99      0.96      0.98       519\n",
      "weighted avg       0.99      0.99      0.99       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "# Here the model is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('buying', 0.1510848831946676), ('maint', 0.2506508516803624), ('doors', 0.060026331736828115), ('persons', 0.19355707150872045), ('lug_boot', 0.09892620952419463), ('safety', 0.2457546523552268)]\n"
     ]
    }
   ],
   "source": [
    "#After training the model##Variable importance is checked, higher values are better\n",
    "print(list(zip(colname,model_DecisionTree.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlot the graph on webgraphviz.com from model_DecisionTree.txt file\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate the file (saved as text in directory) and upload the code from text file in webgraphviz.com to plot the decision tree\n",
    "from sklearn import tree\n",
    "with open(\"model_DecisionTree.txt\", \"w\") as f: #.txt file will be created and stored in python library (C:\\Users\\Manoj Nahak)\n",
    "    #open function is used to open a text file, \"w\" open the file in write mode\n",
    "    f = tree.export_graphviz(model_DecisionTree, feature_names=colname[:-1],out_file=f)\n",
    "    #export_graphviz documents all the steps taking place in the backgroud for a Decision tree\n",
    "\n",
    "#generate the file and upload the code in webgraphviz.com to plot the decision tree\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Plot the graph on webgraphviz.com from model_DecisionTree.txt file\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building using SVM & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the default SVM model\n",
    "# 4 types in Kernal\n",
    "# 1) linear = for less variables\n",
    "# 2) rbf = by default (radial basis function)\n",
    "# 3) Poly\n",
    "# 4) sigmoid = very rarely used\n",
    "from sklearn import svm\n",
    "classifier=svm.SVC(kernel=\"rbf\", gamma=0.1, C=70) #hyperplane adjustment from support vectors is done\n",
    "\n",
    "#fitting training data to the model\n",
    "classifier.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=classifier.predict(X_test) #applying the model\n",
    "#print(list(zip(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 99   2   1   0]\n",
      " [  0  21   0   0]\n",
      " [  0   0 371   0]\n",
      " [  0   0   0  25]]\n",
      "\n",
      "0.9942196531791907\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       102\n",
      "           1       0.91      1.00      0.95        21\n",
      "           2       1.00      1.00      1.00       371\n",
      "           3       1.00      1.00      1.00        25\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       519\n",
      "   macro avg       0.98      0.99      0.98       519\n",
      "weighted avg       0.99      0.99      0.99       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print()\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print()\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "# Here the model accuracy for SVM base model when cost = 1.0 is 85.54% and when cost=70 the accuacy is 99.42%. \n",
    "# Basis tuning the model accuacy improves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#create a model\n",
    "classifier=LogisticRegression()\n",
    "#fitting training data to the model\n",
    "classifier.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=classifier.predict(X_test)\n",
    "#print(list(zip(Y_test,Y_pred))) #O/P will be [(2, 2), (2, 2), (2, 2), (2, 2), (1, 2), (2, 2), (0, 2), (0, 2),......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22   0  80   0]\n",
      " [  3   0  18   0]\n",
      " [ 30   0 341   0]\n",
      " [ 11   0  14   0]]\n",
      "\n",
      "\n",
      "0.6994219653179191\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.22      0.26       102\n",
      "           1       0.00      0.00      0.00        21\n",
      "           2       0.75      0.92      0.83       371\n",
      "           3       0.00      0.00      0.00        25\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       519\n",
      "   macro avg       0.27      0.28      0.27       519\n",
      "weighted avg       0.60      0.70      0.64       519\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "# Here the model accuracy for Logistic Regression base model is 69.94% \n",
    "# Logistic is meant for binary target variable\n",
    "#Here it predicted well for two classes and not for the remaining class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #Bagging Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe never run a single algorithm on the dataset. Multiple algorithms are run on the dataset\\n\\nBasis above example, accuracy is as follows: \\n-DT --> 99.22%\\n-SVC --> 85.58%\\n-Tuned SVC --> 99.42%\\n-lr --> 69.94%\\n\\nTwo approaches are followed in the industry:\\n1) Running individual models --> selecting the best model \\n2) Running combination of models --> averaging the output\\n\\nEnsemble model---\\nBagging : Parallel approach -- Used to improvise over the accuracy score\\n-Bags of data are created simultaneously. Bags of training data is created basis the number of DTs you wish to create.\\n-For each of the bag the observations are selected basis random sampling with replacement. \\n-Each observation has an equal probability of getting selected for all the bags.\\n-The observations will be repeated in the bags. Approx 60% of the training data is selected for each of the bag.\\n-Multiple DTs are run on the individual bags. The model is trained after running on these bags.\\n-Now in case of classification prediction, all the model objects predict each of the observations in the test data and \\nthen finally the mode value of all the model outcomes is taken as the final value/class assigned to observation   \\n-In case of regression prediction, the only difference is that the mean(average) value of all the model outcomes \\nis taken as the final value/class assigned to observation\\n-The number of bags/model should be odd to avoid cases where it is difficult to decide the mode value\\n\\n\\nBagging algorithms:\\nExtraTreesClassifier()\\nRandomForestClassifier()\\n\\nRandom Forest\\nIn case of Random forest, the major difference between bagging and randomforest is that in case of randomforest the observations\\nare sampled as well as the variables are also sampled.\\nSampling is different for both\\nEach data set has randomly sampled observations and different set of variables.\\nMultiple DTs are created\\nIt is time consuming and costly\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We never run a single algorithm on the dataset. Multiple algorithms are run on the dataset\n",
    "\n",
    "Basis above example, accuracy is as follows: \n",
    "-DT --> 99.22%\n",
    "-SVC --> 85.58%\n",
    "-Tuned SVC --> 99.42%\n",
    "-lr --> 69.94%\n",
    "\n",
    "Two approaches are followed in the industry:\n",
    "1) Running individual models --> selecting the best model \n",
    "2) Running combination of models --> averaging the output\n",
    "\n",
    "Ensemble model---\n",
    "Bagging : Parallel approach -- Used to improvise over the accuracy score\n",
    "-Bags of data are created simultaneously. Bags of training data is created basis the number of DTs you wish to create.\n",
    "-For each of the bag the observations are selected basis random sampling with replacement. \n",
    "-Each observation has an equal probability of getting selected for all the bags.\n",
    "-The observations will be repeated in the bags. Approx 60% of the training data is selected for each of the bag.\n",
    "-Multiple DTs are run on the individual bags. The model is trained after running on these bags.\n",
    "-Now in case of classification prediction, all the model objects predict each of the observations in the test data and \n",
    "then finally the mode value of all the model outcomes is taken as the final value/class assigned to observation   \n",
    "-In case of regression prediction, the only difference is that the mean(average) value of all the model outcomes \n",
    "is taken as the final value/class assigned to observation\n",
    "-The number of bags/model should be odd to avoid cases where it is difficult to decide the mode value\n",
    "\n",
    "\n",
    "Bagging algorithms:\n",
    "ExtraTreesClassifier()\n",
    "RandomForestClassifier()\n",
    "\n",
    "Random Forest\n",
    "In case of Random forest, the major difference between bagging and randomforest is that in case of randomforest the observations\n",
    "are sampled as well as the variables are also sampled.\n",
    "Sampling is different for both\n",
    "Each data set has randomly sampled observations and different set of variables.\n",
    "Multiple DTs are created\n",
    "It is time consuming and costly\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting using the Bagging_Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model=(ExtraTreesClassifier(50,random_state=10))  ##first argument specifies the number of bags we want to create\n",
    "#fit the model on the data and predict the values\n",
    "model=model.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100   0   2   0]\n",
      " [  3  18   0   0]\n",
      " [  3   0 368   0]\n",
      " [  1   2   0  22]]\n",
      "\n",
      "\n",
      "0.9788053949903661\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       102\n",
      "           1       0.90      0.86      0.88        21\n",
      "           2       0.99      0.99      0.99       371\n",
      "           3       1.00      0.88      0.94        25\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       519\n",
      "   macro avg       0.96      0.93      0.94       519\n",
      "weighted avg       0.98      0.98      0.98       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "# Here the model accuracy for Logistic Regression base model is 97.88%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting using the Random_Forest_Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model=(RandomForestClassifier(100,random_state=10))  ##first argument specifies the number of bags we want to create\n",
    "#fit the model on the data and predict the values\n",
    "model=model.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 99   3   0   0]\n",
      " [  1  19   0   1]\n",
      " [  2   0 369   0]\n",
      " [  2   0   0  23]]\n",
      "\n",
      "\n",
      "0.9826589595375722\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       102\n",
      "           1       0.86      0.90      0.88        21\n",
      "           2       1.00      0.99      1.00       371\n",
      "           3       0.96      0.92      0.94        25\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       519\n",
      "   macro avg       0.94      0.95      0.95       519\n",
      "weighted avg       0.98      0.98      0.98       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "# Here the model accuracy for Logistic Regression base model is 97.88%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting - Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the model will predect the final output and then we will introduce the testing data to the model\n",
    "#whereas in cas eon bagging we take majority vote to select the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTwo commonly use boosting algorithm\\nAdaBoostClassifier() - for any model (increase the weight of misclassified and decrese the weight of correcltly classified)\\nGradientBoostingClassifier() - invented For tuning the decision tree model & increase the weight of misclassified\\nXGBoost() xtrem gradient boosting\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two commonly use boosting algorithm\n",
    "AdaBoostClassifier() - for any model (increase the weight of misclassified and decrese the weight of correcltly classified)\n",
    "GradientBoostingClassifier() - invented For tuning the decision tree model & increase the weight of misclassified\n",
    "XGBoost() xtrem gradient boosting\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Asaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting using the Adaboost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model_AdaBoost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                                    n_estimators=50, random_state=10)\n",
    "\n",
    "#fit the model on the data and predict the values\n",
    "model_AdaBoost.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=model_AdaBoost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 99   2   1   0]\n",
      " [  4  17   0   0]\n",
      " [  0   0 371   0]\n",
      " [  1   0   0  24]]\n",
      "\n",
      "\n",
      "0.9845857418111753\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       102\n",
      "           1       0.89      0.81      0.85        21\n",
      "           2       1.00      1.00      1.00       371\n",
      "           3       1.00      0.96      0.98        25\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       519\n",
      "   macro avg       0.96      0.94      0.95       519\n",
      "weighted avg       0.98      0.98      0.98       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting using the Gradient boosting classifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_GradientBoosting=GradientBoostingClassifier(n_estimators=200,\n",
    "                                                 min_samples_leaf=3,\n",
    "                                                 random_state=10)\n",
    "\n",
    "#fit the model on the data and predict the values\n",
    "model_GradientBoosting.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred= model_GradientBoosting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101   1   0   0]\n",
      " [  0  21   0   0]\n",
      " [  0   0 371   0]\n",
      " [  0   0   0  25]]\n",
      "\n",
      "\n",
      "0.9980732177263969\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       102\n",
      "           1       0.95      1.00      0.98        21\n",
      "           2       1.00      1.00      1.00       371\n",
      "           3       1.00      1.00      1.00        25\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       519\n",
      "   macro avg       0.99      1.00      0.99       519\n",
      "weighted avg       1.00      1.00      1.00       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBagging/Boosting :- different data but same classifier\\n\\nEnsemble model : -same data but different classifiers\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bagging/Boosting :- different data but same classifier\n",
    "\n",
    "Ensemble model : -same data but different classifiers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the sub models\n",
    "estimators = []\n",
    "#model1 = LogisticRegression()\n",
    "#estimators.append(('log',model1))\n",
    "\n",
    "model2 = DecisionTreeClassifier(random_state=10)\n",
    "estimators.append(('cart',model2))\n",
    "\n",
    "model3 = SVC(kernel='rbf',gamma=0.1,C=70)\n",
    "estimators.append(('svm',model3))\n",
    "\n",
    "#print(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "ensemble.fit(X_train, Y_train)\n",
    "Y_pred=ensemble.predict(X_test)\n",
    "#print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102   0   0   0]\n",
      " [  2  19   0   0]\n",
      " [  0   0 371   0]\n",
      " [  1   0   0  24]]\n",
      "\n",
      "\n",
      "0.9942196531791907\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       102\n",
      "           1       1.00      0.90      0.95        21\n",
      "           2       1.00      1.00      1.00       371\n",
      "           3       1.00      0.96      0.98        25\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       519\n",
      "   macro avg       0.99      0.97      0.98       519\n",
      "weighted avg       0.99      0.99      0.99       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "print()\n",
    "print()\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
